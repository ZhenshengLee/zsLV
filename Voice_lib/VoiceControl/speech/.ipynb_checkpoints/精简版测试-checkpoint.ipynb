{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#需要的包一股脑全放里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2, urllib,sys,os,pynlpir\n",
    "import codecs,re,string,json\n",
    "from subprocess import *\n",
    "from collections import Counter\n",
    "from svmutil import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'\\u4ece', u'preposition'), (u'\\u673a\\u7535', u'distinguishing word'), (u'\\u697c', u'noun'), (u'\\u51fa\\u53d1', u'verb:intransitive verb'), (u'\\u6cbf\\u7740', u'preposition'), (u'\\u7389\\u5e26', u'noun'), (u'\\u8def', u'noun'), (u'\\u8d70', u'verb'), (u'\\u5230', u'verb'), (u'\\u6865', u'noun'), (u'\\u8fb9', u'suffix'), (u'\\u7136\\u540e', u'conjunction'), (u'\\u53f3', u'noun of locality'), (u'\\u8f6c', u'verb')]\n",
      "从/p 机电/b 楼/n 出发/vi 沿着/p 玉带/n 路/n 走/v 到/v 桥/n 边/k 然后/c 右/f 转/v \n"
     ]
    }
   ],
   "source": [
    "#anoconda 安装第三方包  1. conda install  2.解压到~/anaconda/pkgs， python setup.py (develop)\n",
    "pynlpir.open()\n",
    "s = '从机电楼出发沿着玉带路走到桥边然后右转'\n",
    "print pynlpir.segment(s,pos_names='all')\n",
    "from pynlpir import nlpir\n",
    "nlpir.Init(nlpir.PACKAGE_DIR,  nlpir.UTF8_CODE, None)\n",
    "pynlpir.nlpir.ImportUserDict('user_dic.txt')\n",
    "pynlpir.nlpir.FileProcess('nodupliteInput.txt', 'splitNLPIRtrain.txt',True)\n",
    "print pynlpir.nlpir.ParagraphProcess(s, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#分词标注部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#分词与词性标注  以json格式输出\n",
    "def divPos(input):\n",
    "    uri_base = \"http://ltpapi.voicecloud.cn/analysis/?\"\n",
    "    api_key  = \"l3z7P327MjUbYOpUyR7m4kmobn7eVVrsaJlUSxlb\" \n",
    "    text= urllib.quote(input)\n",
    "    #语言云的真正调用方法    \n",
    "    uri_base = \"http://ltpapi.voicecloud.cn/analysis/?\"       \n",
    "    data = {\n",
    "            \"api_key\"   : \"l3z7P327MjUbYOpUyR7m4kmobn7eVVrsaJlUSxlb\",\n",
    "            \"text\"      : text,\n",
    "            \"format\"    : \"json\",\n",
    "            \"has_key\"   : \"false\",\n",
    "            \"pattern\"   : \"pos\",               \n",
    "\n",
    "            }\n",
    "    params = urllib.urlencode(data)\n",
    "    try:\n",
    "        request  = urllib2.Request(uri_base)\n",
    "        response = urllib2.urlopen(request, params)\n",
    "        content  = response.read().strip()\n",
    "        \n",
    "        return content \n",
    "    except urllib2.HTTPError, e:\n",
    "        print >> sys.stderr, e.reason\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#json格式转化成plain格式\n",
    "def jsonToPlain(contentJson):\n",
    "        #只保留分词和标注输出\n",
    "        #posResult=[word for word in sentence[0] for sentence in content for ]\n",
    "        resultList=[]\n",
    "        for paragraph in contentJson:\n",
    "            sentenceList=[]\n",
    "            if paragraph[0][-1][-1]== \"wp\":\n",
    "                for word in paragraph[0][:-1]:\n",
    "                    sentenceList.append(word[1]+'\\t'+word[2])\n",
    "            else:\n",
    "                for word in paragraph[0]:\n",
    "                    sentenceList.append(word[1]+'\\t'+word[2])\n",
    "            sentence='\\n'.join(sentenceList).encode('utf-8')\n",
    "            resultList.append(sentence+'\\n'*2)\n",
    "        \n",
    "        return resultList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#分词调用程序\n",
    "def distinguishWord():\n",
    "    #测试句子，直接输入\n",
    "    dataa=raw_input('输入命令：'.decode('utf-8')).decode('gb2312').encode('utf-8')#程序中字符串为utf-8，交互中键盘敲入为gb2312\n",
    "    #输入句子，调用分词标注函数，输出结果（json）\n",
    "    if dataa=='':\n",
    "        return False\n",
    "    output=divPos(dataa)\n",
    "\n",
    "    #将分词标注结果输出到txt文档中\n",
    "    f1=open('distinguishWordJson.txt','w')\n",
    "    f1.write(output)\n",
    "    f1.close()\n",
    "    \n",
    "    contentJson=json.loads(output)\n",
    "\n",
    "    resultList=jsonToPlain(contentJson)\n",
    "    f2=open('distinguishWordPlain.txt','w')\n",
    "    f2.writelines(resultList)\n",
    "    f2.close()\n",
    "    \n",
    "    return True\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入命令：斯科拉得分能力斯蒂芬\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinguishWord()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从机电楼出发沿着玉带路走到桥边然后右转"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# crf语义标注部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(template,trainTxt,model):      \n",
    "    #returnCode = call('crf_learn.exe template crfTrain1.txt model1')\n",
    "    call('crf_learn.exe '+template+' '+trainTxt+' '+ model)\n",
    "def test(model,testTxt,testOutput):\n",
    "    \n",
    "    #Popen('crf_test.exe -m model1 crfTest1.txt >output1.txt', shell = True, stdout = PIPE).stdout\n",
    "    Popen('crf_test.exe -m '+model+' '+ testTxt +'>'+testOutput, shell = True, stdout = PIPE).communicate()\n",
    "def evaluate(testOutput,testResult):\n",
    "    \n",
    "    evalue=Popen('conlleval.pl -d \"\\t\" -r < '+testOutput+' > '+testResult,\n",
    "                 shell = True, stdout = PIPE).stdout\n",
    "    #https://argcv.com/articles/2104.c#respond   参考资料，参数使用很全\n",
    "    #print evalue.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def semanticTrain():\n",
    "    train('semanticTemplate','semanticTrain.txt','semanticModel')\n",
    "def semanticTest():\n",
    "    test('semanticModel','distinguishWordPlain.txt','semanticOutput.txt')\n",
    "    #test('model','crfTest1.txt','output1.txt')\n",
    "def semanticEvaluate():\n",
    "    evaluate('semanticEvaluate.txt','semanticEvaluateOutput.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "semanticTrain()\n",
    "semanticTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#将名词和方向词合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#将普通的字符串格式的输入转化成json列表形式\n",
    "def plainToJson(semanticOutput):\n",
    "    fCrfTrain=open(semanticOutput,'r')\n",
    "    article=fCrfTrain.read()\n",
    "    if article[:3] == codecs.BOM_UTF8:\n",
    "            article = article[3:]  \n",
    "    fCrfTrain.close()\n",
    "\n",
    "    sentenceLabel=re.compile(r\"(.*?)\\n\\n\",re.S)\n",
    "    sentenceList=re.findall(sentenceLabel,article)\n",
    "\n",
    "    #print sentenceList[0:10]\n",
    "\n",
    "    wordListInSentenceList=[]\n",
    "    for sentence in sentenceList:\n",
    "        wordList=sentence.split('\\n')\n",
    "        wordPosYuyiList=[]\n",
    "        for word in wordList:\n",
    "            wordPosYuyi=word.split('\\t')\n",
    "            wordPosYuyiList.append(wordPosYuyi)\n",
    "        wordListInSentenceList.append(wordPosYuyiList)\n",
    "    return wordListInSentenceList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#将名词和方向词合并 输出json格式\n",
    "def n_f(wordListInSentenceList):\n",
    "    for s in xrange(len(wordListInSentenceList)):\n",
    "        wordPosition=0\n",
    "        while wordPosition<len(wordListInSentenceList[s]):\n",
    "            nPosition=[]\n",
    "            while wordPosition<len(wordListInSentenceList[s]) and '7_' in wordListInSentenceList[s][wordPosition][-1]:\n",
    "                \n",
    "                nPosition.append(wordPosition)\n",
    "                wordPosition+=1\n",
    "            #print nPosition \n",
    "            if len(nPosition)!=0:\n",
    "                #\n",
    "                words=[]\n",
    "                for p in nPosition:\n",
    "                    words.append(wordListInSentenceList[s][p][0])\n",
    "                word=''.join(words)\n",
    "                nTotal=[word,'ns','7']\n",
    "                wordListInSentenceList[s]=wordListInSentenceList[s][:nPosition[0]]+[nTotal]+wordListInSentenceList[s][nPosition[-1]+1:]\n",
    "                #print wordListInSentenceList[s]\n",
    "                wordPosition=nPosition[0]\n",
    "                \n",
    "            fPosition=[]\n",
    "            while wordPosition<len(wordListInSentenceList[s]) and '2_' in wordListInSentenceList[s][wordPosition][-1]:\n",
    "                \n",
    "                fPosition.append(wordPosition)\n",
    "                wordPosition+=1\n",
    "            #print nPosition \n",
    "            if len(fPosition)!=0:\n",
    "                #\n",
    "                words=[]\n",
    "                for p in fPosition:\n",
    "                    words.append(wordListInSentenceList[s][p][0])\n",
    "                word=''.join(words)\n",
    "                fTotal=[word,'nd','2']\n",
    "                wordListInSentenceList[s]=wordListInSentenceList[s][:fPosition[0]]+[fTotal]+wordListInSentenceList[s][fPosition[-1]+1:]\n",
    "                #print wordListInSentenceList[s]\n",
    "                wordPosition=fPosition[0]\n",
    "            #print wordPosition\n",
    "            wordPosition+=1\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#将合并后的json格式转化成plain格式输出    \n",
    "def jsonToPlain1(contentJson):\n",
    "        #只保留分词和标注输出\n",
    "        #posResult=[word for word in sentence[0] for sentence in content for ]\n",
    "        resultList=[]\n",
    "        for paragraph in contentJson:\n",
    "            sentenceList=[]\n",
    "            for word in paragraph:\n",
    "                sentenceList.append(word[0]+'\\t'+word[1]+'\\t'+word[2])\n",
    "            sentence='\\n'.join(sentenceList)\n",
    "            resultList.append(sentence+'\\n'*2)\n",
    "        return resultList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def merge():\n",
    "    wordListInSentenceList=plainToJson('semanticOutput.txt')#crfTrain.txt  包含词  词性  语义 的plain文件\n",
    "    n_f(wordListInSentenceList)\n",
    "    print wordListInSentenceList\n",
    "\n",
    "    data=json.dumps(wordListInSentenceList)\n",
    "    f3=open('n_f_outputTestJson.txt','w')                #jsonData.txt对应crfTrain的合并结果\n",
    "    f3.writelines(data)\n",
    "    f3.close()\n",
    "\n",
    "    result=jsonToPlain1(wordListInSentenceList)\n",
    "    f4=open('n_f_outputTest.txt','w')           #n_f.data对应jsonData.txt  plain格式\n",
    "    f4.writelines(result)\n",
    "    f4.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['\\xe5\\xbe\\x80', 'p', '1'], ['\\xe5\\x89\\x8d', 'nd', '2'], ['\\xe8\\xb5\\xb0', 'v', '3_4'], ['\\xe4\\xba\\x94', 'm', '6'], ['\\xe7\\xb1\\xb3', 'q', '8'], ['\\xe5\\xb7\\xa6\\xe8\\xbd\\xac', 'm', '0']]]\n"
     ]
    }
   ],
   "source": [
    "merge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#svm判断是否"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#所有句子的特征列表\n",
    "def sentProperty(listData):\n",
    "    property_all=[]\n",
    "    for sentence in listData:\n",
    "        #句子中的语义列表\n",
    "        yuyiList=[ word[-1] for word in sentence]\n",
    "        yuyiStr=' '+' '.join(yuyiList)\n",
    "        #除0外的词占句子的比例\n",
    "        property1=1-Counter(yuyiList)['0']/len(yuyiList)\n",
    "        #是否有动词\n",
    "        property2=1 if re.search(r'3_|4',yuyiStr) else 0\n",
    "        \n",
    "        #是否含有方向词\n",
    "        property3=1 if '2' in yuyiList else 0\n",
    "        #是否含有地名词\n",
    "        property4=1 if '7' in yuyiList else 0\n",
    "        #是否含有数词加量词的集合\n",
    "        property5=1 if  re.search(r'6 8',yuyiStr)  else 0\n",
    "        #方向词加动词的个数\n",
    "        property6=len(re.findall(r'2 [34]',yuyiStr))\n",
    "        #动词加地名词的个数\n",
    "        property7=len(re.findall(r'3_? 7| 4 7',yuyiStr))\n",
    "        #地名词加动词\n",
    "        property8=len(re.findall(r'7 [34]',yuyiStr))\n",
    "        #动词的个数\n",
    "        property9=len(re.findall(r'3_| 4',yuyiStr))\n",
    "        #关键词类别个数占句子长度的比例\n",
    "        property10=len(set(yuyiList))/len(yuyiList)\n",
    "        \n",
    "        property_all.append([property1,property2,property3,property4,property5,property6,property7,property8,property9,property10])\n",
    "    return property_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#给每个句子贴上1或-1的标签，自己标出来\n",
    "def label_initialize():\n",
    "    set_label_1=[1]*60\n",
    "    label_0=[7,53,54,55,56,57,58,59]\n",
    "    for label in label_0:\n",
    "        set_label_1[label]=-1\n",
    "    return set_label_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#将句子的特征和1、-1标签弄成svm的输入格式\n",
    "def svm_input(property_all,label_tag):\n",
    "    svm_format=[]\n",
    "    for index in xrange(len(property_all)):               \n",
    "        column=str(label_tag[index])\n",
    "        for property_index in xrange(len(property_all[index])):\n",
    "            column+='\\t'+str(property_index+1)+':'+str(property_all[index][property_index])   \n",
    "        svm_format.append(column+'\\n')\n",
    "    return svm_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def svmProperty(n_f_input):\n",
    "#从合并后的带有语义标注的预料中  提取出svm的特征向量  以及自己标注的标签 存放在一个文本文档中\n",
    "    f5=open(n_f_input,'r')  #jsonData\n",
    "    jsonData1=f5.read()\n",
    "    f5.close()\n",
    "    listData=json.loads(jsonData1)\n",
    "    sentPropertyAll=sentProperty(listData)\n",
    "\n",
    "    #sentLabel=label_initialize()\n",
    "    #sentLabel=[0]*len(sentPropertyAll)\n",
    "    \n",
    "    return sentPropertyAll\n",
    "    \n",
    "    \n",
    "    #svm_format=svm_input(sentPropertyAll,sentLabel)\n",
    "\n",
    "    #f6=open(svm_input,'w')\n",
    "    #f6.writelines(svm_format)\n",
    "    #f6.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#训练模型  \n",
    "def svmTrain():\n",
    "    sentPropertyAll=svmProperty('jsonData.txt')\n",
    "    sentLabel=label_initialize()\n",
    "    m=svm_train(sentLabel,sentPropertyAll,'-c 4')\n",
    "    svm_save_model('svmModel', m)\n",
    "    \n",
    "#进行测试\n",
    "def svmTest():\n",
    "    #y,x=svm_read_problem('svm_input_test.txt')\n",
    "    x=svmProperty('n_f_outputTestJson.txt')\n",
    "    y=[0]*len(x)\n",
    "    m = svm_load_model('svmModel')\n",
    "    p_label,p_acc,p_val=svm_predict(y,x,m)\n",
    "    return p_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svmTrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0% (0/1) (classification)\n",
      "[1.0]\n"
     ]
    }
   ],
   "source": [
    "print svmTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#crf分句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitTrain():\n",
    "    train('splitTemplate','splitTrain.txt','splitModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitTest():\n",
    "    test('splitModel','n_f_outputTest.txt','splitOutput.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#crf起止点识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def startFinishTrain():\n",
    "    train('startFinishTemplate','startFinishTrain.txt','startFinishmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def startFinishTest():\n",
    "    test('startFinishmodel','splitOutput.txt','startFinishOutput.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testAll():\n",
    "    while distinguishWord():\n",
    "        semanticTest()\n",
    "        merge()\n",
    "        \n",
    "        if svmTest()>0:\n",
    "            splitTest()\n",
    "            startFinishTest()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入命令：往前方走十米的距离\n",
      "[[['\\xe5\\xbe\\x80', 'p', '1'], ['\\xe5\\x89\\x8d\\xe6\\x96\\xb9', 'nl', '2'], ['\\xe8\\xb5\\xb0', 'v', '3_2'], ['\\xe5\\x8d\\x81', 'm', '6'], ['\\xe7\\xb1\\xb3', 'q', '8'], ['\\xe7\\x9a\\x84\\xe8\\xb7\\x9d\\xe7\\xa6\\xbb', 'ns', '7']]]\n",
      "Accuracy = 0% (0/1) (classification)\n",
      "输入命令：\n"
     ]
    }
   ],
   "source": [
    "testAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-ba3ee304423d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstartFinishTrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-8b94ee843f96>\u001b[0m in \u001b[0;36mstartFinishTrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mstartFinishTrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'startFinishTemplate'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'startFinishTrain.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'startFinishmodel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: global name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "startFinishTrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
