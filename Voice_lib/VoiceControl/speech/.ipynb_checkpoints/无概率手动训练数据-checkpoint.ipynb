{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2, urllib,sys,os\n",
    "import codecs,re,string,json\n",
    "from subprocess import *\n",
    "from collections import Counter\n",
    "from svmutil import *"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "下面是一些格式转换的东西plain/json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plainToJson(semanticOutput):\n",
    "    fCrfTrain=open(semanticOutput,'r')\n",
    "    article=fCrfTrain.read()\n",
    "    if article[:3] == codecs.BOM_UTF8:\n",
    "            article = article[3:]  \n",
    "    fCrfTrain.close()\n",
    "\n",
    "    sentenceLabel=re.compile(r\"(.*?)\\n\\n\",re.S)\n",
    "    sentenceList=re.findall(sentenceLabel,article)\n",
    "\n",
    "    #print sentenceList[0:10]\n",
    "\n",
    "    wordListInSentenceList=[]\n",
    "    for sentence in sentenceList:\n",
    "        wordList=sentence.split('\\n')\n",
    "        wordPosYuyiList=[]\n",
    "        for word in wordList:\n",
    "            wordPosYuyi=word.split('\\t')\n",
    "            wordPosYuyiList.append(wordPosYuyi)\n",
    "        wordListInSentenceList.append(wordPosYuyiList)\n",
    "    return wordListInSentenceList\n",
    "#普通的字符串格式的输入（带有边缘概率需要处理）转化成json列表形式\n",
    "def proPlainToJson(semanticOutput):\n",
    "    fCrfTrain=open(semanticOutput,'r+')\n",
    "    article=fCrfTrain.read()\n",
    "    if article[:3] == codecs.BOM_UTF8:\n",
    "            article = article[3:]  \n",
    "    fCrfTrain.close()\n",
    "\n",
    "    sentenceLabel=re.compile(r\"(.*?)\\n\\n\",re.S)\n",
    "    sentenceList=re.findall(sentenceLabel,article)\n",
    "\n",
    "    #print sentenceList[0:10]\n",
    "\n",
    "    wordListInSentenceList=[]\n",
    "    for sentence in sentenceList:\n",
    "        wordList=sentence.split('\\n')\n",
    "        \n",
    "        wordPosYuyiProList=[]\n",
    "        for word in wordList[1:]:\n",
    "            wordPosYuyiPro=word.split('\\t')\n",
    "            newWordPosYuyiPro=wordPosYuyiPro[:-1]+[wordPosYuyiPro[-1].split('/')[0]]\n",
    "            wordPosYuyiProList.append(newWordPosYuyiPro)\n",
    "        wordListInSentenceList.append([float(wordList[0][2:])]+wordPosYuyiProList)\n",
    "    return wordListInSentenceList\n",
    "#json格式转化成plain格式\n",
    "def jsonToPlain(contentJson):\n",
    "    #只保留分词和标注输出\n",
    "    #posResult=[word for word in sentence[0] for sentence in content for ]\n",
    "    resultList=[]\n",
    "    for paragraph in contentJson:\n",
    "        sentenceList=[]\n",
    "        if paragraph[0][-1][-1]== \"wp\":\n",
    "            for word in paragraph[0][:-1]:\n",
    "                if u\"转\" in word[1]:\n",
    "                    word[2]=\"v\"\n",
    "                sentenceList.append(word[1]+'\\t'+word[2])\n",
    "        else:\n",
    "            for word in paragraph[0]:\n",
    "                if u\"转\" in word[1]:\n",
    "                    word[2]=\"v\"\n",
    "                sentenceList.append(word[1]+'\\t'+word[2])\n",
    "        sentence='\\n'.join(sentenceList).encode('utf-8')\n",
    "        sentence1=re.sub( r'(nh|ni|nl|ns|nz)' , 'n' , sentence )\n",
    "        resultList.append(sentence1+'\\n'*2)\n",
    "\n",
    "    return resultList\n",
    "def jsonToPlain1(contentJson):\n",
    "    #只保留分词和标注输出\n",
    "    #posResult=[word for word in sentence[0] for sentence in content for ]\n",
    "    resultList=[]\n",
    "    for paragraph in contentJson:\n",
    "        sentenceList=[]\n",
    "        for word in paragraph:\n",
    "            sentenceList.append(word[0]+'\\t'+word[1]+'\\t'+word[2])\n",
    "        sentence='\\n'.join(sentenceList)\n",
    "        resultList.append(sentence+'\\n'*2)\n",
    "    return resultList"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#分词与词性标注  以json格式输出\n",
    "def divPos(input):\n",
    "    uri_base = \"http://ltpapi.voicecloud.cn/analysis/?\"\n",
    "    api_key  = \"l3z7P327MjUbYOpUyR7m4kmobn7eVVrsaJlUSxlb\" \n",
    "    text= urllib.quote(input)\n",
    "    #语言云的真正调用方法    \n",
    "    uri_base = \"http://ltpapi.voicecloud.cn/analysis/?\"       \n",
    "    data = {\n",
    "            \"api_key\"   : \"l3z7P327MjUbYOpUyR7m4kmobn7eVVrsaJlUSxlb\",\n",
    "            \"text\"      : text,\n",
    "            \"format\"    : \"json\",\n",
    "            \"has_key\"   : \"false\",\n",
    "            \"pattern\"   : \"pos\",               \n",
    "\n",
    "            }\n",
    "    params = urllib.urlencode(data)\n",
    "    try:\n",
    "        request  = urllib2.Request(uri_base)\n",
    "        response = urllib2.urlopen(request, params)\n",
    "        content  = response.read().strip()\n",
    "        \n",
    "        return content \n",
    "    except urllib2.HTTPError, e:\n",
    "        print >> sys.stderr, e.reason\n",
    "\n",
    "#分词调用程序\n",
    "def distinguishWord(train_test_material,distinguishWordPlainTrain_Test):\n",
    "    #测试句子，直接输入\n",
    "    if train_test_material==None:\n",
    "        dataa=raw_input('输入命令：'.decode('utf-8')).decode('gb2312').encode('utf-8').replace(',','')#程序中字符串为utf-8，交互中键盘敲入为gb2312\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        f= open(train_test_material, 'r')  #文档得是utf8\n",
    "        dataa=f.read()\n",
    "        if dataa[:3] == codecs.BOM_UTF8:\n",
    "            dataa = dataa[3:]\n",
    "    \n",
    "    output=divPos(dataa) #输入句子，调用分词标注函数，输出结果（json）\n",
    "    \n",
    "    #将分词标注结果输出到txt文档中\n",
    "    contentJson=json.loads(output)\n",
    "\n",
    "    resultList=jsonToPlain(contentJson)\n",
    "    f2=open(distinguishWordPlainTrain_Test,'w')\n",
    "    f2.writelines(resultList)\n",
    "    f2.close()\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#distinguishWord('corpus','distinguishWordPlainTrain') #训练库\n",
    "distinguishWord('corpusAdd','distinguishWordPlainTest') #从文件读入 多个测试语句\n",
    "#distinguishWord(None,'distinguishWordPlainTest') #从键盘输入 单个测试语句"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3层crf的函数,夹杂着svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(template,trainTxt,model):      \n",
    "    #returnCode = call('crf_learn.exe template crfTrain1.txt model1')\n",
    "    call('crf_learn.exe '+template+' '+trainTxt+' '+ model+' -t')\n",
    "def test(model,testTxt,testOutput):\n",
    "    \"\"\"\n",
    "    -v1:输出标签的概率值\n",
    "    -n :输出几层不同概率只的选项 用处不大\n",
    "    -t:输出model的txt版本\n",
    "    -f：是一阀值，只有某词的频率大于该值 才有用\n",
    "    -c：跟拟合度有关的一个参数\n",
    "    -h: 可以随时打开帮助看看\n",
    "    \n",
    "    \"\"\"\n",
    "    Popen('crf_test.exe -m '+model+' '+ testTxt +'>'+testOutput, shell = True, stdout = PIPE).communicate()\n",
    "    #Popen('crf_test.exe -m '+model+' '+ testTxt +'>'+testOutput, shell = True, stdout = PIPE).communicate()\n",
    "\n",
    "\n",
    "def semanticTrain():#http://www.hankcs.com/nlp/the-crf-model-format-description.html  不错的资料\n",
    "    train('semanticTemplate','semanticTrain','semanticModel')\n",
    "def semanticTest():\n",
    "    test('semanticModel','distinguishWordPlainTest','semanticOutput')\n",
    "    #test('model','crfTest1.txt','output1.txt')\n",
    "def splitTrain():\n",
    "    train('splitTemplate','splitTrain','splitModel')\n",
    "\n",
    "def splitTest():\n",
    "    test('splitModel','n_f_outputTest','splitOutput')\n",
    "def startFinishTrain():\n",
    "    train('startFinishTemplate','startFinishTrain','startFinishmodel')\n",
    "def startFinishTest():\n",
    "    test('startFinishmodel','splitOutput','startFinishOutput')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "semanticTrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "splitTrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "startFinishTrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_f(wordListInSentenceList):\n",
    "    for s in xrange(len(wordListInSentenceList)):\n",
    "        wordPosition=0\n",
    "        while wordPosition<len(wordListInSentenceList[s]):\n",
    "            nPosition=[]\n",
    "            while wordPosition<len(wordListInSentenceList[s]) and '7' in wordListInSentenceList[s][wordPosition][-1]:\n",
    "                \n",
    "                nPosition.append(wordPosition)\n",
    "                wordPosition+=1\n",
    "            #print nPosition \n",
    "            if len(nPosition)!=0:\n",
    "                #\n",
    "                words=[]\n",
    "                for p in nPosition:\n",
    "                    words.append(wordListInSentenceList[s][p][0])\n",
    "                word=''.join(words)\n",
    "                nTotal=[word,'n','7']\n",
    "                wordListInSentenceList[s]=wordListInSentenceList[s][:nPosition[0]]+[nTotal]+wordListInSentenceList[s][nPosition[-1]+1:]\n",
    "                #print wordListInSentenceList[s]\n",
    "                wordPosition=nPosition[0]\n",
    "                \n",
    "            vPosition=[]\n",
    "            while wordPosition<len(wordListInSentenceList[s]) and '3' in wordListInSentenceList[s][wordPosition][-1]:\n",
    "                \n",
    "                vPosition.append(wordPosition)\n",
    "                wordPosition+=1\n",
    "            #print nPosition \n",
    "            if len(vPosition)!=0:\n",
    "                #\n",
    "                words=[]\n",
    "                for v in vPosition:\n",
    "                    words.append(wordListInSentenceList[s][v][0])\n",
    "                word=''.join(words)\n",
    "                vTotal=[word,'v','3']\n",
    "                wordListInSentenceList[s]=wordListInSentenceList[s][:vPosition[0]]+[vTotal]+wordListInSentenceList[s][vPosition[-1]+1:]\n",
    "                #print wordListInSentenceList[s]\n",
    "                wordPosition=vPosition[0]\n",
    "                \n",
    "            fPosition=[]\n",
    "            while wordPosition<len(wordListInSentenceList[s]) and '2' in wordListInSentenceList[s][wordPosition][-1]:\n",
    "                \n",
    "                fPosition.append(wordPosition)\n",
    "                wordPosition+=1\n",
    "            #print nPosition \n",
    "            if len(fPosition)!=0:\n",
    "                #\n",
    "                words=[]\n",
    "                for p in fPosition:\n",
    "                    words.append(wordListInSentenceList[s][p][0])\n",
    "                word=''.join(words)\n",
    "                fTotal=[word,'nd','2']\n",
    "                wordListInSentenceList[s]=wordListInSentenceList[s][:fPosition[0]]+[fTotal]+wordListInSentenceList[s][fPosition[-1]+1:]\n",
    "                #print wordListInSentenceList[s]\n",
    "                wordPosition=fPosition[0]\n",
    "            #print wordPosition\n",
    "            wordPosition+=1\n",
    "    return\n",
    "def merge():\n",
    "    wordListInSentenceList=plainToJson('semanticOutput')#crfTrain.txt  包含词  词性  语义 的plain文件\n",
    "    n_f(wordListInSentenceList)\n",
    "\n",
    "\n",
    "    data=json.dumps(wordListInSentenceList)\n",
    "    f3=open('n_f_outputTestJson','w')                #jsonData.txt对应crfTrain的合并结果\n",
    "    f3.writelines(data)\n",
    "    f3.close()\n",
    "\n",
    "    result=jsonToPlain1(wordListInSentenceList)\n",
    "    f4=open('n_f_outputTest','w')           #n_f.data对应jsonData.txt  plain格式\n",
    "    f4.writelines(result)\n",
    "    f4.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "semanticTest()\n",
    "merge()\n",
    "splitTest()\n",
    "startFinishTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
